{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satelite imagery in the Torres Strait (Digital Globe)\n",
    "\n",
    "This document shows the high-resolution DigitalGlobe imagery of the Torres Strait for 2016 and 2017. Two outcomes come from this document. First, it shows how spatialy distributed are the images over the selected zone. Second, it create a visualization tool to capture the more frequent days of collection. For both years recolectation rates are low and not cover the total extension of the Torres Strait area. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Torres Strait (May 2016 - Jun 2017)</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gbdxtools\n",
    "import numpy as np\n",
    "#import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "import shapely as sp\n",
    "from shapely.geometry.polygon import LinearRing, Polygon\n",
    "from shapely.geometry import mapping, shape\n",
    "from gbdx_auth import gbdx_auth\n",
    "import geojson, json\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "\n",
    "#Open a session using the Authentication files (~/.gbdx-config)\n",
    "gbdx = gbdx_auth.get_session()\n",
    "gbdx = gbdxtools.Interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "#Create a area to search and retrieve information\n",
    "torres_strait = \"POLYGON((139.0 -9.0, 145.0 -9.0, 145 -12, 139 -12, 139 -9.0))\"\n",
    "world = \"POLYGON((-180 -90, -180 90, 180 90, 180 -90, -180 -90))\"\n",
    "filters = [\"cloudCover < 30\"]\n",
    "types = ['DigitalGlobeAcquisition']\n",
    "\n",
    "results = gbdx.catalog.search(searchAreaWkt= torres_strait,\n",
    "                              startDate=\"2016-05-01T00:00:00.000Z\",\n",
    "                              endDate=\"2017-06-30T00:00:00.000Z\",\n",
    "                              filters=filters,\n",
    "                              types = types)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Results is a json object (which is already a list, so no need of json.loads). First we can try with the first\n",
    "#element of the json list.\n",
    "a = results[1]['properties']['footprintWkt']\n",
    "a_sp = sp.wkt.loads(a)\n",
    "\n",
    "#We have retrieve this number of images\n",
    "print(\"We have \"+ str(len(results)) + \" images available in the selected area!\")\n",
    "\n",
    "#Now that we have this, we can explore the spatial distribution of the images\n",
    "tiles = []\n",
    "for tile in results:\n",
    "    tiles.append(tile['properties']['footprintWkt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Interactive plot with available imagery\n",
    "from geomet import wkt\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import folium\n",
    "\n",
    "#Create an interactive Leaflet map with the location of the torres strait\n",
    "torres_strait = folium.Map(location = [-10.144989750644969, 142.3181966067051,], \n",
    "                           tiles='OpenStreetMap', \n",
    "                           zoom_start=6\n",
    "                          )\n",
    "\n",
    "#Option 1: Only plot the available images with the same color\n",
    "tiles_geojson = []\n",
    "for img in results:\n",
    "    tiles_geojson.append(wkt.loads(img['properties']['footprintWkt']))\n",
    "    for tile in tiles_geojson:\n",
    "        tile[\"properties\"] = img[\"properties\"]\n",
    "        \n",
    "        \n",
    "style_function = lambda x: {'borderColor': 'rgba(255, 0, 0, 0)'}\n",
    "\n",
    "for tile in tiles_geojson:\n",
    "    folium.GeoJson(tile, style_function=style_function).add_to(torres_strait)\n",
    "        \n",
    "#Option 2: Create a pd DataFrame to make a cloropeth by month\n",
    "df_imgs = list(map(lambda x: x[\"properties\"], results))\n",
    "data_imgs = pd.read_json(json.dumps(df_imgs))\n",
    "\n",
    "#If you want to explore the data dataframe you can print:\n",
    "#print(data_imgs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torres_strait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore time of the images\n",
    "data_imgs[\"timestamp\"] = pd.to_datetime(data_imgs[\"timestamp\"])\n",
    "data_imgs[\"date\"] = pd.DatetimeIndex(data_imgs[\"timestamp\"]).normalize() #This is not needed, is only to learn how to remove time from timestamps\n",
    "data_imgs[\"day\"], data_imgs[\"month\"] = data_imgs[\"timestamp\"].dt.day, data_imgs[\"timestamp\"].dt.month\n",
    "data_imgs[\"hour\"], data_imgs[\"minute\"], data_imgs[\"second\"] = data_imgs[\"timestamp\"].dt.hour, data_imgs[\"timestamp\"].dt.minute, data_imgs[\"timestamp\"].dt.second \n",
    "#print(data_imgs)\n",
    "\n",
    "data_imgs_agg = pd.DataFrame(data_imgs.groupby([\"date\"]).size().rename(\"counts\"))\n",
    "data_imgs_agg[\"date\"] = data_imgs_agg.index\n",
    "data_imgs_agg[\"day\"], data_imgs_agg[\"month\"], data_imgs_agg[\"year\"] = data_imgs_agg[\"date\"].dt.day, data_imgs_agg[\"date\"].dt.month, data_imgs_agg[\"date\"].dt.year \n",
    "data_imgs_agg[\"month_year\"] = data_imgs_agg['month'].map(str)+\"-\"+data_imgs_agg[\"year\"].map(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create pivot table to visualize time\n",
    "data_imgs_piv = data_imgs_agg.pivot(index='day', columns=\"month_year\", values='counts')\n",
    "data_imgs_piv = data_imgs_piv.fillna(0)\n",
    "#print(data_imgs_piv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method taken from: http://nbviewer.jupyter.org/gist/joelotz/5427209 based on FlowingData Graph. \n",
    "\n",
    "# Plot it out\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.pcolor(data_imgs_piv, cmap=plt.cm.Purples, alpha=0.8)\n",
    "\n",
    "##################################################\n",
    "## FORMAT ##\n",
    "##################################################\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,11)\n",
    "\n",
    "# turn off the frame\n",
    "ax.set_frame_on(False)\n",
    "fig.colorbar(heatmap)\n",
    "\n",
    "# put the major ticks at the middle of each cell\n",
    "ax.set_yticks(np.arange(data_imgs_piv.shape[0])+0.5, minor=False)\n",
    "ax.set_xticks(np.arange(data_imgs_piv.shape[1])+0.5, minor=False)\n",
    "\n",
    "# want a more natural, table-like display\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "# Set the labels\n",
    "labels = [\"May\", \"Jun\", \"Jul\", \"Aug\", \"Sept\", \"Oct\", \"Nov\", \"Dec\", 'Jan', \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"]\n",
    "\n",
    "# note I could have used nba_sort.columns but made \"labels\" instead\n",
    "ax.set_xticklabels(labels, minor=False) \n",
    "ax.set_yticklabels(data_imgs_piv.index, minor=False)\n",
    "\n",
    "# rotate the \n",
    "plt.xticks(rotation=90)\n",
    "ax.grid(False)\n",
    "\n",
    "# Turn off all the ticks\n",
    "ax = plt.gca()\n",
    "ax.set_title('Frequency of imagery collection - 2016\\n\\n\\n\\n') \n",
    "plt.savefig('/mnt/data/shared/timeavailable.png')\n",
    "\n",
    "\n",
    "\n",
    "#for t in ax.xaxis.get_major_ticks(): \n",
    "#    t.tick1On = False \n",
    "#    t.tick2On = False \n",
    "#for t in ax.yaxis.get_major_ticks(): \n",
    "#    t.tick1On = False \n",
    "#    t.tick2On = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Export imagery metadata to check if coincides with AIS/SPIRE data\n",
    "data_imgs.to_csv(path_or_buf=\"/mnt/data/shared/imgs_metadata_2017.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Marine areas (May 2016 - Jun 2017)</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from shapely import wkb\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "def url_geojson_to_wkt(url):\n",
    "    #Retrieve data from url\n",
    "    geojson_request = urllib.request.urlopen(url)\n",
    "    geojson_data = json.loads(geojson_request.read().decode())[\"rows\"]\n",
    "\n",
    "    #Open data using shapely and taking it from binary to a valid geometry in Python\n",
    "    def wkb_to_wkt(poly):\n",
    "        poly_sp = wkb.loads(poly, hex=True)\n",
    "        unary_poly = unary_union(poly_sp)\n",
    "        return unary_poly\n",
    "    \n",
    "    geoms = [i[\"the_geom\"] for i in geojson_data]\n",
    "    shapes_wkt = [wkb_to_wkt(x) for x in geoms]\n",
    "    return shapes_wkt \n",
    "\n",
    "\n",
    "marine_areas = url_geojson_to_wkt(\"https://observatory.carto.com/api/v2/sql?q=select%20*%20from%20observatory.whosonfirst_marinearea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def month_dates(year, month):\n",
    "    d = datetime(year, 1, 1)\n",
    "    d = d + relativedelta(months = month)\n",
    "    dlt = relativedelta(months = 1)\n",
    "    return d , d + dlt\n",
    "\n",
    "def week_dates(year, week):\n",
    "    d = datetime(year, 1, 1)\n",
    "    d = d - timedelta(d.weekday())\n",
    "    dlt = timedelta(days = (week-1)*7)\n",
    "    return d + dlt, d + dlt + timedelta(days=6)\n",
    "\n",
    "print(week_dates(2016, 2)[0].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "print(week_dates(2016, 2)[1].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "\n",
    "\n",
    "filters = [\"cloudCover < 30\"]\n",
    "types = ['DigitalGlobeAcquisition']\n",
    "results = []\n",
    "n = 0 \n",
    "\n",
    "for e, i in zip(range(len(marine_areas)), marine_areas):\n",
    "    while True:\n",
    "        try:\n",
    "            if i.geom_type != 'MultiPolygon':\n",
    "                poly_wkt=sp.wkt.dumps(i)\n",
    "                time.sleep(np.random.randint(1, high=10, size=None, dtype='l'))\n",
    "                query_results = gbdx.catalog.search(searchAreaWkt= poly_wkt,\n",
    "                                        startDate=\"2016-05-01T00:00:00.000Z\",\n",
    "                                        endDate=\"2017-06-30T00:00:00.000Z\",\n",
    "                                        filters=filters,\n",
    "                                        types = types)\n",
    "        \n",
    "                if len(query_results) < 1000:\n",
    "                    results.append(query_results)\n",
    "                    print(\"Geometry index: \" + str(e) + \" [\" + str(len(results[n])) + \"]\" + \" - Added!\")\n",
    "                    n += 1\n",
    "                    \n",
    "                else: #Do it by month\n",
    "                    for j in range(4,18):\n",
    "                        time.sleep(np.random.randint(1, high=10, size=None, dtype='l'))\n",
    "                        query_results_month = gbdx.catalog.search(searchAreaWkt= poly_wkt,\n",
    "                                                                  startDate=month_dates(2016, j)[0].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n",
    "                                                                  endDate=month_dates(2016, j)[1].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n",
    "                                                                  filters=filters,\n",
    "                                                                  types = types)\n",
    "                        if len(query_results_month) < 1000:\n",
    "                            results.append(query_results_month)\n",
    "                            print(\"Geometry index: \" + str(e) + \" [\"+str(len(results[n]))+\"] \" + \"-\" + \" Month: \" + str(j))\n",
    "                            n += 1\n",
    "                    \n",
    "                        else: #Do it by week\n",
    "                            for k in range(19,78): \n",
    "                                time.sleep(np.random.randint(1, high=10, size=None, dtype='l'))\n",
    "                                results.append(gbdx.catalog.search(searchAreaWkt= poly_wkt,\n",
    "                                                                   startDate=week_dates(2016, k)[0].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n",
    "                                                                   endDate=week_dates(2016, k+1)[1].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n",
    "                                                                   filters=filters,\n",
    "                                                                   types = types))\n",
    "                        \n",
    "                                print(\"Geometry index: \" + str(e) + \" [\"+str(len(results[n]))+\"] \" + \"-\" + \" Week: \" + str(k-19))\n",
    "                                n += 1\n",
    "    \n",
    "            else:\n",
    "                print(\"Multipolygon, sorry :(\")\n",
    "                \n",
    "                \n",
    "        except requests.exceptions.RequestException:\n",
    "                print(\"Damn! This server </3. Retry!\")\n",
    "                continue\n",
    "                \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum([len(i) for i in results])\n",
    "concat_result_list = [image for query in results for image in query]\n",
    "#Save file - no more requests!\n",
    "with open('/mnt/data/shared/results_gbdx_marine_areas.txt', 'w') as file:\n",
    "    for item in concat_result_list:\n",
    "        file.write(\"%s\\n\" % item)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results=[]\n",
    "with open('/mnt/data/shared/gbdx/results_gbdx_marine_areas.txt') as json_file:\n",
    "    for line in json_file:\n",
    "        results.append(eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We have retrieve this number of images\n",
    "print(\"We have \"+ str(len(results)) + \" images available in the selected area!\")\n",
    "\n",
    "#Now that we have this, we can explore the spatial distribution of the images\n",
    "tiles = []\n",
    "for tile in results:\n",
    "    tiles.append(tile['properties']['footprintWkt'])\n",
    "\n",
    "#Extract properties from images and get a pandas object from it\n",
    "df_imgs = list(map(lambda x: x[\"properties\"], results))\n",
    "data_imgs = pd.read_json(json.dumps(df_imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    sys.stdout.write('\\r')\n",
    "    # the exact output you're looking for:\n",
    "    sys.stdout.write(\"[%-99s] %d%%\" % ('='*i, i))\n",
    "    sys.stdout.flush()\n",
    "    sleep(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Explore time of the images\n",
    "data_imgs[\"timestamp\"] = pd.to_datetime(data_imgs[\"timestamp\"])\n",
    "#print(data_imgs[\"timestamp\"])\n",
    "data_imgs[\"date\"] = pd.DatetimeIndex(data_imgs[\"timestamp\"]).normalize() #This is not needed, is only to learn how to remove time from timestamps\n",
    "data_imgs['year'], data_imgs[\"day\"], data_imgs[\"month\"], data_imgs[\"hour\"], data_imgs[\"minute\"], data_imgs[\"second\"] = data_imgs['timestamp'].dt.year, data_imgs[\"timestamp\"].dt.day, data_imgs[\"timestamp\"].dt.month, data_imgs[\"timestamp\"].dt.hour, data_imgs[\"timestamp\"].dt.minute, data_imgs[\"timestamp\"].dt.second\n",
    "\n",
    "#Save raw data to sql\n",
    "#print(data_imgs)\n",
    "#data_imgs.to_csv(\"/mnt/data/shared/gbdx/imgs_metadata_2017.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Aggregate data\n",
    "data_imgs_agg = pd.DataFrame(data_imgs.groupby([\"date\"]).size().rename(\"counts\"))\n",
    "data_imgs_agg[\"date\"] = data_imgs_agg.index\n",
    "data_imgs_agg[\"day\"], data_imgs_agg[\"month\"], data_imgs_agg[\"year\"] = data_imgs_agg[\"date\"].dt.day, data_imgs_agg[\"date\"].dt.month, data_imgs_agg[\"date\"].dt.year \n",
    "data_imgs_agg[\"month_year\"] = data_imgs_agg['month'].map(str)+\"-\"+data_imgs_agg[\"year\"].map(str)\n",
    "\n",
    "data_imgs_piv = data_imgs_agg.pivot(index='day', columns=\"month_year\", values='counts')\n",
    "data_imgs_piv = data_imgs_piv.fillna(0)\n",
    "#print(data_imgs_piv)\n",
    "\n",
    "#Method taken from: http://nbviewer.jupyter.org/gist/joelotz/5427209 based on FlowingData Graph. \n",
    "\n",
    "# Plot it out\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.pcolor(data_imgs_piv, cmap=plt.cm.Purples, alpha=0.8)\n",
    "\n",
    "\n",
    "##################################################\n",
    "## FORMAT ##\n",
    "##################################################\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,11)\n",
    "\n",
    "# turn off the frame\n",
    "ax.set_frame_on(False)\n",
    "fig.colorbar(heatmap)\n",
    "\n",
    "# put the major ticks at the middle of each cell\n",
    "ax.set_yticks(np.arange(data_imgs_piv.shape[0])+0.5, minor=False)\n",
    "ax.set_xticks(np.arange(data_imgs_piv.shape[1])+0.5, minor=False)\n",
    "\n",
    "# want a more natural, table-like display\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "\n",
    "# Set the labels\n",
    "# label source:https://en.wikipedia.org/wiki/Basketball_statistics\n",
    "labels = [\"May\", \"Jun\", \"Jul\", \"Aug\", \"Sept\", \"Oct\", \"Nov\", \"Dec\", 'Jan', \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"]\n",
    "\n",
    "\n",
    "# note I could have used nba_sort.columns but made \"labels\" instead\n",
    "ax.set_xticklabels(labels, minor=False) \n",
    "ax.set_yticklabels(data_imgs_piv.index, minor=False)\n",
    "\n",
    "# rotate the ticks\n",
    "plt.xticks(rotation=90)\n",
    "ax.grid(False)\n",
    "\n",
    "# Turn off all the ticks\n",
    "ax = plt.gca()\n",
    "ax.set_title('Frequency of imagery collection - 2017\\n\\n\\n\\n') \n",
    "plt.savefig('/mnt/data/shared/ ')\n",
    "\n",
    "\n",
    "#for t in ax.xaxis.get_major_ticks(): \n",
    "#    t.tick1On = False \n",
    "#    t.tick2On = False \n",
    "#for t in ax.yaxis.get_major_ticks(): \n",
    "#    t.tick1On = False \n",
    "#    t.tick2On = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1><center> Marine areas (May 2016 - Jun 2017)</center></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_geojson_to_wkt(url):\n",
    "    #Retrieve data from url\n",
    "    geojson_request = urllib.request.urlopen(url)\n",
    "    geojson_data = json.loads(geojson_request.read().decode())[\"rows\"]\n",
    "\n",
    "    #Open data using shapely and taking it from binary to a valid geometry in Python\n",
    "    def wkb_to_wkt(poly):\n",
    "        poly_sp = wkb.loads(poly, hex=True)\n",
    "        unary_poly = unary_union(poly_sp)\n",
    "        return unary_poly\n",
    "    \n",
    "    geoms = [i[\"the_geom\"] for i in geojson_data]\n",
    "    shapes_wkt = [wkb_to_wkt(x) for x in geoms]\n",
    "    return shapes_wkt \n",
    "\n",
    "oceans = url_geojson_to_wkt(\"https://observatory.carto.com:443/api/v2/sql?q=select%20*%20from%20observatory.whosonfirst_ocean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-05-02T00:00:00.000000Z\n",
      "Geometry index: 0 [112] - Added!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f51089cc5868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mpoly_wkt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwkt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m540\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                     query_results = gbdx.catalog.search(searchAreaWkt= poly_wkt,\n\u001b[1;32m     26\u001b[0m                                                         \u001b[0mstartDate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mday_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%dT%H:%M:%S.%fZ\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def day_dates(year, day):\n",
    "    d = datetime(year, 1, 1)\n",
    "    d = d + relativedelta(days = day)\n",
    "    dlt = relativedelta(days = 1)\n",
    "    return d , d + dlt\n",
    "\n",
    "filters = [\"cloudCover < 30\"]\n",
    "types = ['DigitalGlobeAcquisition']\n",
    "results = []\n",
    "n = 0 \n",
    "\n",
    "for e, i in zip(range(len(oceans)), oceans):\n",
    "    while True:\n",
    "        try:\n",
    "            if i.geom_type != 'MultiPolygon':\n",
    "                poly_wkt=sp.wkt.dumps(i)\n",
    "                for j in range(121, 540):\n",
    "                    time.sleep(np.random.randint(1, high=10, size=None, dtype='l'))\n",
    "                    query_results = gbdx.catalog.search(searchAreaWkt= poly_wkt,\n",
    "                                                        startDate= day_dates(2016, j)[0].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n",
    "                                                        endDate= day_dates(2016, j)[1].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n",
    "                                                        filters=filters,\n",
    "                                                        types = types)\n",
    "                    \n",
    "                    if len(query_results) < 1000:\n",
    "                        results.append(query_results)\n",
    "                        print(day_dates(2016, j)[1].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "                        print(\"Geometry index: \" + str(e) + \" [\" + str(len(results[n])) + \"]\" + \" - Added!\")\n",
    "                        n += 1\n",
    "                    \n",
    "                    else: #Sorry, no more\n",
    "                        print(\"Too much images!\")\n",
    "                        \n",
    "            else: #Break the Multipolygons\n",
    "                for k in i.geoms:\n",
    "                    for j in range(121, 540):\n",
    "                        poly_wkt=sp.wkt.dumps(k)\n",
    "                        time.sleep(np.random.randint(1, high=10, size=None, dtype='l'))\n",
    "                        query_results = gbdx.catalog.search(searchAreaWkt= poly_wkt,\n",
    "                                                            startDate= day_dates(2016, j)[0].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n",
    "                                                            endDate= day_dates(2016, j)[1].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n",
    "                                                            filters=filters,\n",
    "                                                            types = types)\n",
    "                        if len(query_results) < 1000:\n",
    "                            results.append(query_results)\n",
    "                            print(day_dates(2016, j)[1].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "                            print(\"Geometry index: \" + str(e) + str(k) + \" [\" + str(len(results[n])) + \"]\" + \" - Added!\")\n",
    "                            n += 1\n",
    "                            \n",
    "                        else: #Sorry, no more\n",
    "                            print(\"Too much images!\")\n",
    "                    \n",
    "                \n",
    "                print(\"Multipolygon, sorry :(\")\n",
    "                \n",
    "                \n",
    "        except requests.exceptions.RequestException:\n",
    "                print(\"Damn! This server </3. Retry!\")\n",
    "                continue\n",
    "                \n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-05-01T00:00:00.000000Z\n",
      "2016-05-02T00:00:00.000000Z\n",
      "2017-06-24T00:00:00.000000Z\n",
      "2017-06-25T00:00:00.000000Z\n"
     ]
    }
   ],
   "source": [
    "def day_dates(year, day):\n",
    "    d = datetime(year, 1, 1)\n",
    "    d = d + relativedelta(days = day)\n",
    "    dlt = relativedelta(days = 1)\n",
    "    return d , d + dlt\n",
    "\n",
    "print(day_dates(2016, 121)[0].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "print(day_dates(2016, 121)[1].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "\n",
    "\n",
    "print(day_dates(2016, 540)[0].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"))\n",
    "print(day_dates(2016, 540)[1].strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum([len(i) for i in results])\n",
    "concat_result_list = [image for query in results for image in query]\n",
    "#Save file - no more requests!\n",
    "with open('/mnt/data/shared/results_gbdx_ocean_areas.txt', 'w') as file:\n",
    "    for item in concat_result_list:\n",
    "        file.write(\"%s\\n\" % item)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.3 :: Anaconda custom (64-bit)\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
